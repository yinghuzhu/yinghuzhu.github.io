<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一凡之客</title>
  
  
  <link href="https://www.yzhu.name/atom.xml" rel="self"/>
  
  <link href="https://www.yzhu.name/"/>
  <updated>2022-01-21T07:07:58.069Z</updated>
  <id>https://www.yzhu.name/</id>
  
  <author>
    <name>yhzhu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>消息的可靠投递</title>
    <link href="https://www.yzhu.name/2021/04/13/reliable-message-delivery/"/>
    <id>https://www.yzhu.name/2021/04/13/reliable-message-delivery/</id>
    <published>2021-04-13T09:43:57.000Z</published>
    <updated>2022-01-21T07:07:58.069Z</updated>
    
    <content type="html"><![CDATA[<p>在大型互联网项目中我们经常会用到消息队列(简称MQ)；主要用在异步消息，应用解耦，流量削锋等场景；在实际应用中经常需要保证消息的可靠投递<span id="more"></span>,即：不能丢消息。</p><h3 id="MQ消费术语"><a href="#MQ消费术语" class="headerlink" title="MQ消费术语"></a>MQ消费术语</h3><pre><code>为了保证保证消息的可靠投递，先了解一下MQ的消费术语</code></pre><ul><li><p>At Lest Once(至少一次)<br> 消息在消费的过程中，至少被发送到一个consumer; 如果consumer处理失败，会再次发送到consumer； 所以消息可能会重复被消费，通常会要求consumer保证幂等性</p></li><li><p>At Most Once(至多一次)<br> 消息在消费的过程中，至多被一个consumer消费; 如果consumer处理失败，消息可能会丢失；只能用于可接受消息丢失的场景  </p></li><li><p>Exactly Once(仅有一次)<br>发送到消息系统的消息只能被消费端处理且仅处理一次，即使生产端重试消息发送导致某消息重复投递，该消息在消费端也只被消费一次。常用MQ产品没有太多理想的实现 。</p></li></ul><h3 id="消息的可靠投递"><a href="#消息的可靠投递" class="headerlink" title="消息的可靠投递"></a>消息的可靠投递</h3><p>我们以<a href="https://www.rabbitmq.com/">RabbitMQ</a>为例从以下几个方面保证说明如何实现消息的可靠投递</p><h4 id="消息中间件可靠性"><a href="#消息中间件可靠性" class="headerlink" title="消息中间件可靠性"></a>消息中间件可靠性</h4><ul><li>使用RabbitMQ的集群模式，basic模式和镜像模式都可以保证消息不丢失；根据业务特点在吞吐量和高可用之间权衡选择合适的集群模式</li><li>确保exchange和queue的持久化<h4 id="生产端可靠性"><a href="#生产端可靠性" class="headerlink" title="生产端可靠性"></a>生产端可靠性</h4></li><li>发送消息时候设置消息持久化属性<strong>DeliverModel</strong> </li><li>消息持久化到数据库状态为发送中</li><li>消息到达exchange开启confirm模式，生产者发送完消息后等待broker的ack，超过一定时间没有收到broker的ack后启动job重试，直到收到broker的ack，数据库消息状态更新为已发送</li><li>消息未到达队列开启回退机制，如果消息从exchange无法投递到队列，回调通知调用者，调用者可以重试<h4 id="消费端的可靠性"><a href="#消费端的可靠性" class="headerlink" title="消费端的可靠性"></a>消费端的可靠性</h4></li><li>消费端确保消息处理逻辑的幂等性</li><li>关闭自动ack</li><li>消费端每处理完一次消息后手动回复ack; 如果处理消息程失败回复nack, 默认情况下broker会继续下发消息直到消费成功；注意：消息处理失败必须回复nack，否则broker认为该consumer节点down，不再下发消息到该节点</li></ul><h4 id="监控报警"><a href="#监控报警" class="headerlink" title="监控报警"></a>监控报警</h4><p>对中间件，存储系统，应用等增加监控，在出现问题的时候能第一时间发现</p><p>综上所述，为了保证消息的可靠投递我们需要从开发和运维的角度去考虑；确保每一个环节都能可靠。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在大型互联网项目中我们经常会用到消息队列(简称MQ)；主要用在异步消息，应用解耦，流量削锋等场景；在实际应用中经常需要保证消息的可靠投递&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    <category term="分布式" scheme="https://www.yzhu.name/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>一个三年前的Bug导致上线失败</title>
    <link href="https://www.yzhu.name/2020/05/30/History-Bug-And-Code-Review/"/>
    <id>https://www.yzhu.name/2020/05/30/History-Bug-And-Code-Review/</id>
    <published>2020-05-30T03:16:26.000Z</published>
    <updated>2022-01-19T07:06:58.601Z</updated>
    
    <content type="html"><![CDATA[<p>上周某核心服务上线过程中内存瞬间飙升，导致上线失败。经查是一个三年前的bug引发的。<span id="more"></span></p><h3 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h3><ol><li>17:10：开始发布B组 </li><li>17:20：开始出现内存，接口超时报警；开发和SA开始介入</li><li>17:25：认为是新版本有问题立刻回滚</li><li>回滚后持续半小时问题没有出现</li></ol><h3 id="初步分析"><a href="#初步分析" class="headerlink" title="初步分析"></a>初步分析</h3><p>根据开发反馈，本次上线功能逻辑非常简单，只有20多行代码，而且测试过程中没发现任何问题；于是，大家开始review那段代码，review后得出一致多结论，该代码没问题。逻辑非常简单，只有一个地方调用了一个方法；这个方法已经存在了好你年了，而且被多个场景在调用；另外，这个方法逻辑复杂，团队内没有人了解。大家一致认为这个方法不会有问题，毕竟经历了「时间的考验」。</p><h3 id="重现"><a href="#重现" class="headerlink" title="重现"></a>重现</h3><p>在测试环境又做了一轮压力测试，一切正常。根据目前点情况看来这个bug跟数据有关；就是说，可能是某条数据触发了这个bug的临界条件.所以，我们决定在生产环境发布一台机器做测试; 这次发布后5分钟后出现了内存飙升现象,马上切掉该机器的流量而且dump Jvm内存 。</p><h3 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h3><p>拿到dump文件后立刻启动MAT分析；发现占用内存最多一个对象有3000多万个，分析该对象的调用盏都指向了同一行代码；「这段代码正是经历过时间考验的代码」。review这行代码相关部分马上发现有死循环的问题，只是满足这个条件的数据不容易出现。而这次的数据正好满足了这个条件。找到了原因，解决方法就很容易了；修改完代码review后没问题，测试，上线后一切正常；至此，该问题解决。这里可能有人会说死循环一般会首先导致CPU报警，怎么没看到我们提到CPU到问题；因为这台机器是16核CPU，所以一个核跑满并没有触发报警规则 。</p><h3 id="后续措施"><a href="#后续措施" class="headerlink" title="后续措施"></a>后续措施</h3><ol><li>必须严格执行code review，从流程上去控制</li><li>需要监控各项指标到同比变化 </li></ol><h3 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h3><p>我们总是强调code review到重要性，但执行的时候总有这样那样的原因导致落实不彻底；我不了解当时的情况，但这次的问题我相信只要做过code review是一定可以发现的。另外，我始终相信代码质量最终要依赖开发人员来保证，测试只能保证功能性问题。特别是一些非功能性的边界条件，只能依赖开发保证。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;上周某核心服务上线过程中内存瞬间飙升，导致上线失败。经查是一个三年前的bug引发的。&lt;/p&gt;</summary>
    
    
    
    
    <category term="线上问题" scheme="https://www.yzhu.name/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Sharding-JDBC(ShardingSphere)多数据库与springboot集成 -- 读写分离</title>
    <link href="https://www.yzhu.name/2020/04/11/ShardingSphere-Springboot/"/>
    <id>https://www.yzhu.name/2020/04/11/ShardingSphere-Springboot/</id>
    <published>2020-04-11T10:54:30.000Z</published>
    <updated>2022-01-19T07:06:58.612Z</updated>
    
    <content type="html"><![CDATA[<p>Sharding-JDBC(ShardingSphere)作为分布式数据库中间件,在Java的JDBC层提供的额外服务。它使用客户端直连数据库，以jar包形式提供服务。引入<code>sharding-jdbc-spring-boot-starter</code>可以快速完成与springboot的集成; 但是默认情况下只能支持到一个数据库，如果支持多数据库，需要做部分扩展；本文记录支持多数据库所做的扩展。<span id="more"></span></p><h3 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;4.0.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="扩展部分"><a href="#扩展部分" class="headerlink" title="扩展部分"></a>扩展部分</h3><ul><li><p> 实现<code>EnvironmentAware</code>接口 ，在方法<code>setEnvironment</code>解析properties配置可以获取所有配置信息</p></li><li><p>获取以<code>spring.ydal</code>为前缀的所有数据库配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, Object&gt; databases = PropertyUtil.handle(environment, &quot;spring.ydal&quot; , Map.class);</span><br></pre></td></tr></table></figure></li><li><p>构造每个数据库的数据库的MasterSlave数据源</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> Map&lt;String, Object&gt; dataSourceProps = PropertyUtil.handle(environment, &quot;spring.eldal.&quot; + dataBaseName+&quot;datasource&quot;, Map.class);</span><br><span class="line"> Preconditions.checkState(!dataSourceProps.isEmpty(), &quot;Wrong datasource properties!&quot;);</span><br><span class="line"></span><br><span class="line"> DataSource datasource = DataSourceUtil.getDataSource(dataSourceProps.get(&quot;type&quot;).toString(), dataSourceProps);</span><br><span class="line"> Optional&lt;DataSourcePropertiesSetter&gt; dataSourcePropertiesSetter = DataSourcePropertiesSetterHolder.getDataSourcePropertiesSetterByType(dataSourceProps.get(&quot;type&quot;).toString());</span><br><span class="line"> if (dataSourcePropertiesSetter.isPresent()) &#123;</span><br><span class="line">     dataSourcePropertiesSetter.get().propertiesSet(environment, prefix, dataSourceName, datasource);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">masterSlaveRule = PropertyUtil.handle(environment, &quot;spring.eldal.&quot; + dataBaseName+&quot;.masterslave&quot;, YamlMasterSlaveRuleConfiguration.class);</span><br><span class="line">MasterSlaveRuleConfiguration msConfig = new MasterSlaveRuleConfigurationYamlSwapper().swap(masterSlaveRule);</span><br><span class="line"></span><br><span class="line">Properties dsProps = PropertyUtil.handle(environment, &quot;spring.eldal.&quot; + dataBaseName+&quot;.props&quot;, Properties.class);</span><br><span class="line">MasterSlaveDataSourceFactory.createDataSource(datasource, msConfig, dsProps)</span><br></pre></td></tr></table></figure></li></ul><h3 id="禁用ShardingSphere的Springboot的自动配置"><a href="#禁用ShardingSphere的Springboot的自动配置" class="headerlink" title="禁用ShardingSphere的Springboot的自动配置"></a>禁用ShardingSphere的Springboot的自动配置</h3><p>实现该扩展后以jar包的形式提供出去供多个项目使用，所以我们必须在jar里面禁用ShardingSphere的Springboot的自动配置；可以实现如下接口:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public class YdalAutoConfigurationImportFilter implements AutoConfigurationImportFilter &#123;</span><br><span class="line">    private static final Set&lt;String&gt; SHOULD_SKIP = new HashSet&lt;&gt;(</span><br><span class="line">            Arrays.asList(&quot;org.apache.shardingsphere.shardingjdbc.spring.boot.SpringBootConfiguration&quot;));</span><br><span class="line">    @Override</span><br><span class="line">    public boolean[] match(String[] autoConfigurationClasses, AutoConfigurationMetadata autoConfigurationMetadata) &#123;</span><br><span class="line">        boolean[] matches = new boolean[autoConfigurationClasses.length];</span><br><span class="line"></span><br><span class="line">        for(int i = 0; i&lt; autoConfigurationClasses.length; i++) &#123;</span><br><span class="line">            matches[i] = !SHOULD_SKIP.contains(autoConfigurationClasses[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        return matches;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>将<code>YdalAutoConfigurationImportFilter</code>加入<code>META-INF/spring.factories</code></li></ul><h5 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h5><ul><li>详细代码： <a href="https://github.com/yinghuzhu/ydal-spring-boot-starter">https://github.com/yinghuzhu/ydal-spring-boot-starter</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;Sharding-JDBC(ShardingSphere)作为分布式数据库中间件,在Java的JDBC层提供的额外服务。它使用客户端直连数据库，以jar包形式提供服务。引入&lt;code&gt;sharding-jdbc-spring-boot-starter&lt;/code&gt;可以快速完成与springboot的集成; 但是默认情况下只能支持到一个数据库，如果支持多数据库，需要做部分扩展；本文记录支持多数据库所做的扩展。&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    <category term="分布式" scheme="https://www.yzhu.name/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>JVM初探</title>
    <link href="https://www.yzhu.name/2020/03/29/JVM-NEW/"/>
    <id>https://www.yzhu.name/2020/03/29/JVM-NEW/</id>
    <published>2020-03-29T03:30:40.000Z</published>
    <updated>2022-01-19T07:06:58.601Z</updated>
    
    <content type="html"><![CDATA[<p>最近经常有人跟我一起讨论JVM的相关话题，所以将以前分享过的一篇JVM的基础知识在这里分享出来，欢迎大家指正.<span id="more"></span></p><div class="pdfobject-container" data-target="/pdf/jvm.pdf" data-height="500px"></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近经常有人跟我一起讨论JVM的相关话题，所以将以前分享过的一篇JVM的基础知识在这里分享出来，欢迎大家指正.&lt;/p&gt;</summary>
    
    
    
    
    <category term="JVM" scheme="https://www.yzhu.name/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>Open Tracing(Jaeger) 遭遇多线程</title>
    <link href="https://www.yzhu.name/2019/12/27/Jaeger-Thread/"/>
    <id>https://www.yzhu.name/2019/12/27/Jaeger-Thread/</id>
    <published>2019-12-27T10:33:50.000Z</published>
    <updated>2022-01-19T07:06:58.601Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道在Java技术体系中，链路跟踪严重依赖ThreadLocal；因此在多线程的场景下会导致链路跟踪失效.<span id="more"></span></p><h3 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h3><p>前几天一位开发同学反馈了一个问题，在链路跟踪UI上看到某个链路Rpc的span数量比实际调用少了很多；我听完第一反映是我们最近升级的SDK出问题了? 根据该同学反馈该接口是一个新接口，上线后一直没关注过链路；我们随即在链路跟踪UI上将多个系统的常用接口都检查了一遍发现一切正常，基本排除了SDK可能引起的问题。接着开始检查开发同学的代码,顺着该请求发现代码里面用到了<strong>线程池</strong> ,该同学解释该接口内部需要多次调用多个Rpc接口，为了提升效率所以采用了多线程。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>分析opentracing的源码发现tracing相关的的信息保存在<code>io.opentracing.util.ThreadLocalScopeManager</code>的ThreadLocal变量里:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalScopeManager</span> <span class="keyword">implements</span> <span class="title">ScopeManager</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ThreadLocal&lt;ThreadLocalScope&gt; tlsScope = <span class="keyword">new</span> ThreadLocal&lt;ThreadLocalScope&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Scope <span class="title">activate</span><span class="params">(Span span, <span class="keyword">boolean</span> finishOnClose)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ThreadLocalScope(<span class="keyword">this</span>, span, finishOnClose);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Scope <span class="title">active</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> tlsScope.get();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这就可以解释为什么多线程中的Rpc请求没有将相关Tracing信息传递下去;</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li><p>多线程中ThreadLocal变量的传递:<br>关于多线程中ThreadLocal变量的传递我们可以用阿里巴巴的<a href="https://github.com/alibaba/transmittable-thread-local">transmittable-thread-local</a> ,所以整个思路就是自己实现一遍<code>ThreadLocalScopeManager</code>用<code>TransmittableThreadLocal</code>替换<code>ThreadLocal</code>，以及跟它关联的类<code>ThreadLocalScope</code>(它里面申明了<code>ThreadLocalScopeManager</code>变量)； </p></li><li><p>修改Opentrcing里面默认的<code>ThreadLocalScopeManager</code>为用户自定义<code>ThreadLocalScopeManager</code><br>检查Jaeger的Springboot自动配置文件<code>io.opentracing.contrib.java.spring.jaeger.starter.JaegerAutoConfiguration</code>源码, 在构造Bean <code>io.opentracing.Tracer</code>时有用户自定义方法: <code>tracerCustomizers.forEach(c -&gt; c.customize(builder))</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JaegerAutoConfiguration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Autowired(required = false)</span></span><br><span class="line">  <span class="keyword">private</span> List&lt;TracerBuilderCustomizer&gt; tracerCustomizers = Collections.emptyList();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Bean</span></span><br><span class="line">  <span class="keyword">public</span> io.opentracing.<span class="function">Tracer <span class="title">tracer</span><span class="params">(Sampler sampler,</span></span></span><br><span class="line"><span class="params"><span class="function">                                      Reporter reporter,</span></span></span><br><span class="line"><span class="params"><span class="function">                                      Metrics metrics,</span></span></span><br><span class="line"><span class="params"><span class="function">                                      JaegerConfigurationProperties properties)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> JaegerTracer.Builder builder =</span><br><span class="line">        <span class="keyword">new</span> JaegerTracer.Builder(properties.getServiceName())</span><br><span class="line">            .withReporter(reporter)</span><br><span class="line">            .withSampler(sampler)</span><br><span class="line">            .withTags(properties.determineTags())</span><br><span class="line">            .withMetrics(metrics);</span><br><span class="line"></span><br><span class="line">    tracerCustomizers.forEach(c -&gt; c.customize(builder));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> builder.build();</span><br><span class="line">  &#125;</span><br><span class="line">  .....</span><br></pre></td></tr></table></figure><p>于是，我们只需要自定义<code>TracerBuilderCustomizer</code>调用<code>builder.withScopeManager</code>方法即可</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TtlTracerBuilderCustomizer</span> <span class="keyword">implements</span> <span class="title">TracerBuilderCustomizer</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">customize</span><span class="params">(JaegerTracer.Builder builder)</span> </span>&#123;</span><br><span class="line">        builder.withScopeManager(<span class="keyword">new</span> TracingScopeManager());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在SpringBoot自动配置文件中构造Bean:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;TracerBuilderCustomizer&gt; <span class="title">tracerCustomizers</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Arrays.asList(<span class="keyword">new</span> TtlTracerBuilderCustomizer());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此，问题解决； 代码参考<a href="https://github.com/yinghuzhu/request-tracing">request-tracing</a></p></li></ol><h3 id="关于多线程"><a href="#关于多线程" class="headerlink" title="关于多线程"></a>关于多线程</h3><p>我不建议在互联网高并发请求接口内部采用多线程；以Java技术体系为例，请求到达系统后一般会有容器(Tomcat之类)或者Rpc框架先接收，然而这些框架本来就是多线程在运行,如果系统本来已经到瓶颈了，即使增加线程也不会提升效率；如果系统需要增加线程，首先我们应该增加容器或者Rpc框架的线程数量；另外如果接口性能差，我们首先应该考虑是Sql的问题还是代码逻辑的问题；或者系统达到了瓶颈是否可以通过增加机器提升性能；如果接口逻辑本身太复杂，可能是我们的方案或者设计有问题，或许可以考虑按照离线请求的模式设计接口。</p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>这个问题虽然没有对业务造成影响，但收集到链路肯定是有问题的；这也暴露出了我们开发过程中的一些问题，对非功能性验证做的不到位，这方面还需要加强。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;我们知道在Java技术体系中，链路跟踪严重依赖ThreadLocal；因此在多线程的场景下会导致链路跟踪失效.&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    <category term="链路跟踪" scheme="https://www.yzhu.name/tags/%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA/"/>
    
  </entry>
  
  <entry>
    <title>Java与Go之间gRPC调用失败问题排查</title>
    <link href="https://www.yzhu.name/2019/06/02/gRPC-Go-Java/"/>
    <id>https://www.yzhu.name/2019/06/02/gRPC-Go-Java/</id>
    <published>2019-06-02T01:29:15.000Z</published>
    <updated>2022-01-21T07:37:39.121Z</updated>
    
    <content type="html"><![CDATA[<p>最近接手了一个新项目，被告知Java代码的gRPC客户端无法访问当前项目中Go代码的gRPC服务。通过问题的排查和部分代码改造个人对gRPC的理解更深入了一层.<span id="more"></span></p><h3 id="项目背景："><a href="#项目背景：" class="headerlink" title="项目背景："></a>项目背景：</h3><p>接手该项目后跟该项目相关人员多次沟通后得到如下背景信息:</p><ol><li>该项目Go服务端采用的是2016年9月以前发布的gRPC ；</li><li>项目组对官方提供的Go语言gRPC类库做了部分改造，但，由于改造该代码的工程师已经离职，目前没有人知道具体修改了什么​；</li><li>由于项目周期紧张，没有富余的人力排查该问题；</li><li>目前Java与Go互相交互的接口不到10个，双方先采用HTTP交互;</li><li>服务端总是需要维护两套接口，HTTP和gRPC接口，链路跟踪等公共组件都要适配两套接口，增加了维护成本</li></ol><h3 id="问题重现"><a href="#问题重现" class="headerlink" title="问题重现"></a>问题重现</h3><ol><li>采用Java版本gRPC-Java-1.18.0访问现有的Go服务，请求发出以后被hang住没有任何反馈直到客户端设置的连接超时后断开，Java客户端和Go服务端没有任何输出信息；</li><li>抓包看包文发现在完成TCP的3次握手后，客户端发送了第一HTTP2的包文后，没有收到服务端的任何回复，整个过程被hang；初步估计是双方采用的HTTP2协议不兼容。</li><li>采用低版本的Java gRPC, 调研发现Java的gRPC在2016年9月以前的最新版本是1.0.0，所以从1.0.0开始测试，最终发现0.9.0可以完成正常请求。</li></ol><h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><p>调研发现0.9.0版本的Java gRPC依赖了4.1.0.Beta6的Netty，该Netty版本发布与2015年9月， 而HTTP2在2015年有多次修订，版本号也升级到了17；我估计该版本的Netty是根据HTTP2修订版17以前的规范实现的,而且该版本为Beta版，导致跟其它语言的协议存在某些差异。</p><h3 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h3><p>采用0.9.0版本的gRPC后会带来以下新问题：</p><ol><li>现有的Java项目框架采用springcloud2.0.0，该版本依赖了Netty的4.1.27.Final版本，项目集成后会出现版本冲突导致各种异常出现；</li><li>0.9.0版本的gRPC缺少很多重要的特性，比如：nameresolver，负载均衡 等 …</li><li>最终还是要升级到新版本</li></ol><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案:"></a>解决方案:</h3><p>综合沟通下来最好的方案就是Go服务端升级gRPC到新版本 ，但是团队将官方代码改造后已经无法做到无缝升级，必须要做部分改造才能投入生产​；目前生产环境运行的Go服务超过50+，而且大部分是核心服务；改造后需要充分的测试，总体成本太大，而且目前没有更多的资源投入。最终决定才有用低版本的Java gRPC​，采用以下的措施应对低版本面临的问题:</p><ol><li>所有的Java项目提供统一的parent pom文件，所有的版本信息在parent pom的dependencyManagement中维护；</li><li>增加新功能：nameresolver，负载均衡，兼容Go服务的服务发现，等…</li></ol><h3 id="一点思考"><a href="#一点思考" class="headerlink" title="一点思考"></a>一点思考</h3><p>我们在技术选型的不能过于保守也不能过于激进；太过保守不能充分利用新技术的优势，甚至会影响到团队士气；太过激进会遭遇太多的不确定性，面临着遇到问题没有先例可以参考，遭遇新的bug，等；所以，在团队没有足够的技术支撑的情况下，我不太建议生产环境采用1.0以下的版本。作为技术人员应该随时关注业界的新技术，但在生产环境采用新技术前必须考虑清楚所面对的各种风险，包括以后的升级能否做到无缝升级。​</p><h5 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料:"></a>参考资料:</h5><p><a href="https://grpc.io/">https://grpc.io/</a><br><a href="https://github.com/grpc/grpc-java">https://github.com/grpc/grpc-java</a><br><a href="https://datatracker.ietf.org/doc/draft-ietf-httpbis-http2/history/">https://datatracker.ietf.org/doc/draft-ietf-httpbis-http2/history/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近接手了一个新项目，被告知Java代码的gRPC客户端无法访问当前项目中Go代码的gRPC服务。通过问题的排查和部分代码改造个人对gRPC的理解更深入了一层.&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    <category term="gRPC" scheme="https://www.yzhu.name/tags/gRPC/"/>
    
  </entry>
  
  <entry>
    <title>技术给业务赋能</title>
    <link href="https://www.yzhu.name/2019/04/27/Tech-VS-Biz/"/>
    <id>https://www.yzhu.name/2019/04/27/Tech-VS-Biz/</id>
    <published>2019-04-27T07:40:16.000Z</published>
    <updated>2022-01-19T07:06:58.612Z</updated>
    
    <content type="html"><![CDATA[<p>技术人员在技术选型的时候如何做好技术与业务的平衡，如何推广技术方案，甚至与产品人员沟通改变某些功能的实现方式，这些问题值得每一个技术人员思考。我们不管采用什么技术方案，最终目的是为了帮助业务发展，使公司在商业上获取回报。如果技术人员能有一些产品和运营的思维，对整个业务的发展会起到更好的效果。<span id="more"></span></p><h3 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h3><p>技术选型我们要考虑很多因素：产品所处的阶段，技术栈，当前的技术实力，技术债务，时间,等… 。特别是产品的不同阶段其目标也不同：</p><ol><li><p>初创阶段：这个阶段重点考虑两个方面：</p><ul><li>1.1 <strong>快</strong>：这个时候重点是<strong>快速上线</strong>，快速验证。同时，技术人员应该注意到有些需求产品人员没有提出来，但以后肯定有这个需求，只是当前优先级比较低。所以技术人员应该考虑到这些需求对现有方案可能存在的影响；如果是举手之劳的事情就顺便实现，至少在设计表结构时应该考虑到。可能有人会说该阶段到底哪些需求应该考虑，哪些不应该考虑；我个人的想法是支撑半年内业务的发展不需要重构为目标。所以在出方案前一定要跟产品，运营等业务方充分沟通拿到所有的数据，比如：半年后PV到多少，单量是多少等业务数据，以此为目标。</li><li>1.2 <strong>可维护性</strong>：产品第一版上线后，会持续迭代和bug修复；从可维护性的角度应该考虑这几点：<ul><li>当前团队有相应的技术储备</li><li>业界有大量的工程师正在使用的技术</li><li>如果是开源项目,其社区足够庞大，如果是商业项目需要有充分的技术支持</li><li>一定是让团队内部最资深的工程师开始写第一行代码，因为所有的项目在重构前，代码一定会越写越烂 </li></ul></li></ul></li><li><p>成长阶段 ：这个阶段产品的商业模式已经被验证过，产品正在为大量的用户提供服务，而且业务正在快速发展中，此时的技术方案以不影响现有业务为前提，或者说将影响降到最低，就是行业内所说的<strong>给飞行中的飞机换引擎</strong> 。 个人认为此阶段的技术方案应该重点考虑<strong>可落地</strong> ,一般这个阶段团队也会扩大，同时会引入一些外部人才。这个时候会出现一些新的思想，新的方案，此时要特别注意这些方案是不是立足于本团队的实际情况。特别一些大厂背景的工程师，可能会给出一些高大上的方案，但是各项成本可能非常高。即便是一些规模相当友商团队的工程师过来给出的方案也不一定能马上落地，因为没有一家企业的流程，制度，文化跟另一家企业是完全一样的，技术方案本质是用技术的手段解决业务的问题，流程的问题，质量的问题，效率的问题，成本的问题。</p></li></ol><h3 id="技术方案推广"><a href="#技术方案推广" class="headerlink" title="技术方案推广"></a>技术方案推广</h3><p>有人说一个技术方案的效果只有在实施后才能知道，我个人认为一个技术方案如果推广成本太高一定不是一个好的方案。在出方案时候就应该考虑到如何去推广，一般情况下开发团队时间很紧迫，不可能为了一个技术改造耽误太多时间。我个人的做法是在出方案前跟相关团队沟通清楚当前面对的问题，以及各个团队的诉求。根据具体的问题给出适合的方案，接下来跟自己的领导沟通方案以及部分细节，确保能得到领导的支持「这点非常重要」；再准备方案，包括但不限于相应的文档，代码，工具，流程等；在正式推广之前，召集所有团队相关负责人「甚至所有技术人员」宣讲，主要包括几个方面：面对的问题，应对方案，如何实施，相关团队如何配合与执行，达到的效果，deadline 。特别是团队配合与执行部分给出详细的执行步骤，以及常见的QA。要站在执行团队的立场上考虑问题，让执行团队充分意识到，采用了新方案后可以提升效率，提升质量或者节省成本，等；可能经过多次沟通后仍有部分团队不能配合执行，此时只能将问题上升到更高一层管理者来协调 。技术方案的实施要充分权衡成本与业务的影响，比如一个案例：业务方要求实施方案不允许停机，然而跟业务方沟通告知不停机的成本太高，最终选择了凌晨业务低峰时期停机20分钟来完成，实际实施的过程中真正停机的时间只有5分钟；然而，该时间段对业务的影响也非常低，因此最终的方案都是权衡各方的利益后博弈的结果。方案实施后要监控各项指标，查看各项指标是否符合预期，如果没有达到预期目标，一定要找到根本原因是方案本身的问题，还是执行过程的问题，避免相同的问题再次出现。</p><h3 id="与产品人员沟通"><a href="#与产品人员沟通" class="headerlink" title="与产品人员沟通"></a>与产品人员沟通</h3><p>我们经常看到一些关于产品人员和技术人员相爱相杀的段子，我认为现实中这样的例子并不多「也许我经历少」。技术人员拿到PRD后要仔细分析PRD背后的逻辑和诉求，有些看似简单需求背后可能需要复杂的技术支撑，不是每一个产品人员都有技术背景，此时需要技术人员跟产品人员充分沟通该需求的实现成本「理论上来说所有的需求都是可以实现的」，帮助产品人员梳理出重点和优先级，适当的时候可以减少需求或者改变产品逻辑。切忌一句“这是一个伪需求”或“这个需求实现不了” ；如果认为是伪需求请给出具体的数据，一切以数据为依据 ，即使实现不了也应该告知当前的困难，是资源问题，还是时间的问题等。如果多次沟通都没效果建议换个产品人员伺候，如果你没有选择的余地或者你认为公司都是这种产品人员，建议还是换工作吧。</p><h3 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h3><p>技术的世界比较简单，确定的输入一定可以得到确定的输出，然而，我们的世界并不总是这样，有很多东西是没有绝对的对错之分。技术人员可以经常与非技术人员聊聊，听听他们看问题的角度，听听他们的诉求；也许这样可以让技术更好的帮助业务成长。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;技术人员在技术选型的时候如何做好技术与业务的平衡，如何推广技术方案，甚至与产品人员沟通改变某些功能的实现方式，这些问题值得每一个技术人员思考。我们不管采用什么技术方案，最终目的是为了帮助业务发展，使公司在商业上获取回报。如果技术人员能有一些产品和运营的思维，对整个业务的发展会起到更好的效果。&lt;/p&gt;</summary>
    
    
    
    
    <category term="职场" scheme="https://www.yzhu.name/tags/%E8%81%8C%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>分布式调度系统选型</title>
    <link href="https://www.yzhu.name/2019/03/30/Schedule-Job/"/>
    <id>https://www.yzhu.name/2019/03/30/Schedule-Job/</id>
    <published>2019-03-30T05:56:45.000Z</published>
    <updated>2022-01-19T07:06:58.609Z</updated>
    
    <content type="html"><![CDATA[<h4 id="为什么需要调度系统"><a href="#为什么需要调度系统" class="headerlink" title="为什么需要调度系统"></a>为什么需要调度系统</h4><p>我们可能有这样的需求:</p><ol><li>在某个指定的时间点执行一个任务, 比如凌晨对前一天的数据进行汇总;</li><li>在某个操作后的指定时刻执行某一操作，比如：电商下单后一小时没有支付的订单需要被取消; </li><li>微服务失败后补偿操作;<span id="more"></span></li></ol><p>类似的需求我们都可以通过定时任务去完成。当然对于需求2，我们也可以利用MQ的延时投递功能去实现；但是，与外部系统交互只能用定时任务。</p><h4 id="Java领域主要分布式调度系统"><a href="#Java领域主要分布式调度系统" class="headerlink" title="Java领域主要分布式调度系统"></a>Java领域主要分布式调度系统</h4><ol><li>xxl-job：是一个轻量级分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展 。</li><li>Elastic-Job： 当当开源的分布式调度解决方案，由两个相互独立的子项目Elastic-Job-Lite和Elastic-Job-Cloud组成；Elastic-Job-Lite定位为轻量级无中心化解决方案，使用jar包的形式提供分布式任务的协调服务；Elastic-Job-Cloud采用自研Mesos Framework的解决方案，额外提供资源治理、应用分发以及进程隔离等功能；</li><li>Saturn：是唯品会开源的一个分布式任务调度平台，在当当开源的Elastic Job基础上，取代传统的Linux Cron/Spring Batch Job的方式，做到全域统一配置，统一监控，任务高可用以及分片并发处理；</li><li>light-task-scheduler：阿里员工开源的个人项目,主要用于解决分布式任务调度问题，支持实时任务，定时任务和Cron任务。有较好的伸缩性，扩展性，健壮稳定性</li><li>Quartz: Java定时任务的标配。利用数据库的锁机制实现集群调度，业务代码需要考虑调度的逻辑，对业务代码有入侵。</li></ol><p>可能还有一些其它的系统个人不太了解，或者长期不在维护，它不在我们讨论的范畴。在这些系统中以xxl-job和Elastic-Job影响力最大，所以在调研的过程中以这两个系统为主.</p><h4 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h4><ol><li><p>xxl-job架构 </p><p><img src="/2019/03/30/Schedule-Job/xxl-job.png" alt="xxl-job架构"></p></li><li><p>Elastic-Job-Lite架构</p><p><img src="/2019/03/30/Schedule-Job/Elastic-Job-Lite.png" alt="Elastic-Job-Lite架构"></p></li></ol><p>从两个系统的架构上可以看出最大的区别是xxl-job有中心化的调度中心，所有的job由调度中心统一调度。而Elastic-Job-Lite没有统一调度中心，由每个Job各自任务节点通过选举算法选出调度节点，确切的说各个job之间没有统一的调度中心 ；Elastic-Job-Cloud依赖Mesos完成调度，资源隔离等工作。</p><h4 id="主要功能特性"><a href="#主要功能特性" class="headerlink" title="主要功能特性"></a>主要功能特性</h4><table><thead><tr><th></th><th>xxl-job</th><th>Elastic-Job</th></tr></thead><tbody><tr><td><strong>调度中心HA</strong></td><td>Y</td><td>Y</td></tr><tr><td><strong>执行器HA</strong></td><td>Y</td><td>Y</td></tr><tr><td><strong>弹性扩容缩容</strong></td><td>Y</td><td>Y</td></tr><tr><td><strong>失效转移</strong></td><td>Y</td><td>Y</td></tr><tr><td><strong>任务失败重试</strong></td><td>Y</td><td>Y</td></tr><tr><td><strong>作业分片一致性</strong></td><td>Y</td><td>Y</td></tr><tr><td><strong>资源隔离</strong></td><td>Y</td><td>Y</td></tr><tr><td><strong>多语言</strong></td><td>Y</td><td>N</td></tr><tr><td><strong>权限管理</strong></td><td>N</td><td>N</td></tr></tbody></table><p>从主要功能看上去两者都能提供很好的支持；在多语言支持方面，xxl-job提供通用HTTP任务Handler,业务方只需要提供HTTP链接即可，不限制语言。Elastic-Job目前只能支持Java，其它语言支持需要自行扩展。</p><h4 id="社区支持"><a href="#社区支持" class="headerlink" title="社区支持"></a>社区支持</h4><p>对于开源项目的选择，社区支持是非常重要的因素；强大的社区意味着，在遇到问题的时候可以得到社区资源的支持，否则只能自己解决了；社区支持我们以github的数据为准:</p><table><thead><tr><th></th><th>xxl-job</th><th>Elastic-Job</th></tr></thead><tbody><tr><td><strong>Star</strong></td><td>7408</td><td>4707</td></tr><tr><td><strong>Fork</strong></td><td>3121</td><td>2216</td></tr><tr><td><strong>Contributers</strong></td><td>17</td><td>17</td></tr><tr><td><strong>Open Issues</strong></td><td>82</td><td>115</td></tr><tr><td><strong>Close Issues</strong></td><td>602</td><td>384</td></tr><tr><td><strong>Latest Updated</strong></td><td>24天前</td><td>一年前</td></tr><tr><td><strong>已登记使用的公司</strong></td><td>163</td><td>63</td></tr></tbody></table><p>从以上数据可以看出来，xxl-job在社区支持方面有更大优势 。</p><h4 id="学习成本"><a href="#学习成本" class="headerlink" title="学习成本"></a>学习成本</h4><ul><li>xxl-job可以做到开箱即用，配置信息在管理控制台有UI统一配置，更容易上手 ; 包括监控，报警都可以在管理控制台完成。</li><li>Elastic-Job配置信息在执行器的spring xml文件完成，监控，报警需要自己定制 。</li></ul><h4 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h4><p>从两者的架构可以发现，在大任务量的情况下，Elastic-Job性能要高于xxl-job 。</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>  如果对于技术实力比较强大的团队，而且对性能要求比较搞的情况下，建议选择Elastic-Job，在该基础上进行定制开发，可以取得更好效果；对于技术实力不足，性能能要求不是很高，而且希望快速开发，快速上线建议选择了xxl-job 。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="http://www.xuxueli.com/xxl-job">http://www.xuxueli.com/xxl-job</a><br><a href="http://elasticjob.io/">http://elasticjob.io</a><br><a href="https://vipshop.github.io/Saturn">https://vipshop.github.io/Saturn</a><br><a href="https://github.com/ltsopensource/light-task-scheduler">https://github.com/ltsopensource/light-task-scheduler</a></p>]]></content>
    
    
    <summary type="html">&lt;h4 id=&quot;为什么需要调度系统&quot;&gt;&lt;a href=&quot;#为什么需要调度系统&quot; class=&quot;headerlink&quot; title=&quot;为什么需要调度系统&quot;&gt;&lt;/a&gt;为什么需要调度系统&lt;/h4&gt;&lt;p&gt;我们可能有这样的需求:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在某个指定的时间点执行一个任务, 比如凌晨对前一天的数据进行汇总;&lt;/li&gt;
&lt;li&gt;在某个操作后的指定时刻执行某一操作，比如：电商下单后一小时没有支付的订单需要被取消; &lt;/li&gt;
&lt;li&gt;微服务失败后补偿操作;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    <category term="定时任务" scheme="https://www.yzhu.name/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>海恩法则与生产Bug</title>
    <link href="https://www.yzhu.name/2019/03/01/product-bug/"/>
    <id>https://www.yzhu.name/2019/03/01/product-bug/</id>
    <published>2019-03-01T14:03:43.000Z</published>
    <updated>2022-01-19T07:06:58.613Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2019/03/01/product-bug/heinrich-law.jpeg"><br>「海恩法则」是航空涡轮发动机的发明者帕布斯·海恩提出一个在航空界关于飞行安全的法则。海恩指出：每一起严重事故的背后，必然有29次轻微事故和300起未遂先兆以及1000起事故隐患。法则强调两点：一是事故的发生是量的积累的结果；二是再好的技术，再完美的规章，在实际操作层面，也无法取代人自身的素质和责任心。「海恩法则」虽然针对的是飞行领域，在软件开发领域遭遇生产bug<span id="more"></span>，用「海恩法则」也可以解释。</p><p>当生产环境出现bug的时候，通常情况下，我们会很快定位出bug产生的原因具体在哪一行代码上，然后根据实际情况决定回滚或者修复。然而事后总结发现，每一个线上bug的出现绝不仅仅是代码的问题，会涉及到开发、测试和运维多个环节；更多暴露出的是流程的问题，管理的问题，执行力的问题。特别是初创团队和架构大规模升级后最容易暴露出代码以外的问题。以我个人处理过的生产bug，事后分析原因最多的一次有14项措施要么缺失，要么执行不到位，其中与代码相关的只有2项，更多的原因是方案和流程执行不到位。</p><p>上线前通常会采取一些措施来保证质量；比如：开发阶段的code review，ut以及测试阶段的压测等；而且会有配套的流程确保必要的步骤都执行到位；然而即便采取多么复杂的流程也不能避免bug的出现。归根结底，代码是人写的，是人就可能出错 ；我们要做的<strong>不是不出bug，而是不出低级bug</strong> 。对于可能出现的低级bug要擅于通过工具发现；诚然，再好的工具和流程也比不上人自身的素质和责任心。</p><p>上线后系统会有各种维度的监控确保系统正常运行；在出现生产bug前监控系统通常会有异常表现，比如CPU，内存，IO，线程等指标可能会有同比变化；此时报警策略的精准性和人的责任心就比较重要；发现异常后第一时间根据各项指标分析出异常的根本原因，是正常波动，是受到攻击还是程序bug。特别是新功能或者bug修复后上线要特别注意这些指标。在确认系统出现问题时候后立刻采取相应措施，回滚，扩容，限流，熔断等，避免或者尽可能减少造成的损失 。</p><p>其实所有的问题都可以归结为人的问题。最后想到奈飞文化准则的第一条「我们只招成年人」 。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2019/03/01/product-bug/heinrich-law.jpeg&quot;&gt;&lt;br&gt;「海恩法则」是航空涡轮发动机的发明者帕布斯·海恩提出一个在航空界关于飞行安全的法则。海恩指出：每一起严重事故的背后，必然有29次轻微事故和300起未遂先兆以及1000起事故隐患。法则强调两点：一是事故的发生是量的积累的结果；二是再好的技术，再完美的规章，在实际操作层面，也无法取代人自身的素质和责任心。「海恩法则」虽然针对的是飞行领域，在软件开发领域遭遇生产bug&lt;/p&gt;</summary>
    
    
    
    
    <category term="线上问题" scheme="https://www.yzhu.name/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"/>
    
    <category term="职场" scheme="https://www.yzhu.name/tags/%E8%81%8C%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>服务化多项目测试环境隔离</title>
    <link href="https://www.yzhu.name/2019/02/13/Multi-Project-Test-Env/"/>
    <id>https://www.yzhu.name/2019/02/13/Multi-Project-Test-Env/</id>
    <published>2019-02-13T13:33:10.000Z</published>
    <updated>2022-01-19T07:06:58.603Z</updated>
    
    <content type="html"><![CDATA[<p>单体应用依赖比较少，大部分情况我们只需要启动一个应用就可以开始测试工作。架构升级到服务化后，每个应用依赖比较多，任何一个依赖有问题都会影响测试结果；如果服务化环境中多项目并行测试，测试效率会更差 。 <span id="more"></span> 为此，我们给出服务化多项目并行测试方案。</p><h3 id="版本号隔离"><a href="#版本号隔离" class="headerlink" title="版本号隔离"></a>版本号隔离</h3><p>首先能想到的方法就是利用版本号的特征「类dubb的RPC框架都有版本号属性」，对不同的项目给出不同的版本号; 比如有如下调用关系:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Client -&gt; SA  </span><br></pre></td></tr></table></figure><p>在启动SA的时候对不同的项目指定不同的版本号,Client在调用SA的时候根据不同的项目指定与SA相匹配的版本号.</p><ul><li>优点: 简单，无需额外开发，类dubbo的RPC框架都支持 </li></ul><h3 id="多项目开发测试现状"><a href="#多项目开发测试现状" class="headerlink" title="多项目开发测试现状"></a>多项目开发测试现状</h3><p>实际项目中的调用关系： </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Client -&gt; SA -&gt; SB -&gt; SC -&gt; SD</span><br></pre></td></tr></table></figure><p>有两个项目在并行开发：</p><ul><li>project1「以下简称P1」代码变更部分是SA, SC</li><li>project2「以下简称P2」代码变更部分是SB, SD</li></ul><p>如果我们采用「版本号隔离」方案; 对于P1, 不但需要部署SA和SC，还需要部署Client和SB，因为Client调用SA，SB调用SC的<strong>版本号分别指向新部署的SA和SC</strong> 。P2也需要类似的部署方法；这样我们发现，几乎所有的服务在每个项目中都需要部署一套；造成资源大量浪费，显然不现实。 因此，我们有了下面的方案「动态路由」 。</p><h3 id="动态路由"><a href="#动态路由" class="headerlink" title="动态路由"></a>动态路由</h3><p>动态路由的思路是环境分<strong>一套稳定环境和每个项目对应一套项目测试环境,其中稳定环境包括所有的服务，项目测试环境仅包括该项目代码变更部分的服务</strong>,各个环境的所有服务采用同一注册中心；方案如下图所示 </p><p><img src="/2019/02/13/Multi-Project-Test-Env/multi-test-env.png" alt="服务化多项目测试环境"></p><ul><li>稳定环境: 部署所有的服务，保证稳定环境作为完整的系统；可以利用CI/CD每日从Master拉稳定版本的代码完成自动部署</li><li>P1: project1测试环境，仅部署有代码变更的SA1和SC1 </li><li>P2: project2测试环境，仅部署有代码变更的SB2和SD2</li><li>路由策略： Client发起调用的时候携带该调用链路所有测试环境的路由规则，如果没有指定路由规则请求默认环境的服务 </li></ul><h5 id="实现要点"><a href="#实现要点" class="headerlink" title="实现要点"></a>实现要点</h5><ul><li>每个服务启动时候在注册中心注册服务所在节点测试环境类型(P1，P2或者默认环境)，默认情况注册为默认环境 </li><li>自定义loadbalance，根据路由规则将RPC请求路由到指定节点上</li><li>每次RPC调用将该调用链路的路由规则隐士传递到下层服务</li><li>每个测试环境需要一个独立的入口，以WEB项目为例可以将nginx作为入口，路由规则可以配置在nginx的配置文件中</li><li>对接发布系统，有变更的服务发布到测试环境时候，该测试环境的路由规则自动写入测试环境的入口配置文件</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;单体应用依赖比较少，大部分情况我们只需要启动一个应用就可以开始测试工作。架构升级到服务化后，每个应用依赖比较多，任何一个依赖有问题都会影响测试结果；如果服务化环境中多项目并行测试，测试效率会更差 。&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>服务器自动配置shadowsocks</title>
    <link href="https://www.yzhu.name/2019/01/20/shadowsocks-auto-config/"/>
    <id>https://www.yzhu.name/2019/01/20/shadowsocks-auto-config/</id>
    <published>2019-01-20T06:30:42.000Z</published>
    <updated>2022-01-19T07:06:58.614Z</updated>
    
    <content type="html"><![CDATA[<p>最近几个朋友买了vps后让我帮忙配置shadowsocks ;于是我写了一个小工具在CentOS上完成shadowsocks的自动配置 .<span id="more"></span></p><ol><li><p>购买VPS, 这个根据自己的需要选购合适的VPS厂商购买即可, 操作系统选择CentOS 。</p></li><li><p>以root身份登陆系统 。</p></li><li><p>执行如下命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://www.yzhu.name/tools/install_shadowsocks.sh -s --output install_shadowsocks.sh  &amp;&amp; chmod +x install_shadowsocks.sh</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./install_shadowsocks.sh</span><br></pre></td></tr></table></figure></li></ol><ul><li><p>看到如下提示，可以输入端口号或者直接回车采用默认端口号:8388</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input server port [8388]</span><br></pre></td></tr></table></figure></li><li><p> 看到如下提示，输入密码,默认密码是:123456 ；<strong>强烈建议此处输入密码</strong>。</p></li></ul>   <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input server password [123456]</span><br></pre></td></tr></table></figure><p>   至此如果没有出错，服务端自动配置已完成 。</p><ol start="4"><li><p>客户端配置</p><p>从<a href="https://shadowsocks.org/en/download/clients.html">官网</a> 下载适合自己操作系统的客户端安装，按照下图所示配置:</p><p><img src="/2019/01/20/shadowsocks-auto-config/client_cfg.png" alt="客户端配置"></p></li><li><p>访问<a href="https://www.google.com/">google</a></p><p><img src="/2019/01/20/shadowsocks-auto-config/google.png" alt="google"></p></li></ol><p>至此整个配置结束。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近几个朋友买了vps后让我帮忙配置shadowsocks ;于是我写了一个小工具在CentOS上完成shadowsocks的自动配置 .&lt;/p&gt;</summary>
    
    
    
    
    <category term="工具" scheme="https://www.yzhu.name/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>服务化架构升级实践</title>
    <link href="https://www.yzhu.name/2019/01/03/servcie-deploy-summary/"/>
    <id>https://www.yzhu.name/2019/01/03/servcie-deploy-summary/</id>
    <published>2019-01-03T13:25:29.000Z</published>
    <updated>2022-01-19T07:06:58.614Z</updated>
    
    <content type="html"><![CDATA[<p>目前我们大部分业务都接入了服务化，在过去将近一年的时间里，我们踩了很多坑，也出现了几次生产事故，同时，从某种意义上讲，我们也做了某些「微创新」，使得架构更适合我们团队的实际情况。从目前的结果来看，基本达成了预定目标； <span id="more"></span></p><h3 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h3><p>在前期技术选型的时候，我们调研了<a href="http://dubbo.apache.org/">Dubbo</a>和<a href="https://spring.io/projects/spring-cloud">Spring Cloud</a>，主要从以下这些方面考虑各自的优缺点；最终选择以<a href="http://dubbo.apache.org/">Dubbo</a>为基础技术框架：</p><ol><li>社区支持</li><li>生态建设</li><li>服务治理</li><li>服务监控</li><li>与现有系统的集成</li><li>代码迁移成本</li><li>团队的流程</li><li>运维成本</li><li>团队的经验</li><li>学习成本</li></ol><h3 id="服务化SDK"><a href="#服务化SDK" class="headerlink" title="服务化SDK"></a>服务化SDK</h3><p>考虑到当时<a href="http://dubbo.apache.org/">Dubbo</a>刚开始进入Apache孵化器，也方便我们以后升级；我们提供了封装<a href="http://dubbo.apache.org/">Dubbo</a>的服务化SDK给开发团队使用。我们扩展了如下功能: </p><ol><li>启动完成标示，单个JVM中确保所有服务启动完成而且在注册中心注册</li><li>集成<a href="https://github.com/ctripcorp/apollo">Apollo</a> ，开发团队不用关心基础组件配置信息 </li><li>集成监控，保证服务化后项目必须接入监控</li><li>集成限流降级系统，确保所有的provider必须接入流降级功能</li><li>定向指定服务节点，测试环境中多项目测试的情况下，可以对运行中的服务动态指定provider工作节点 。(我们也注意到官方在2.7版本的tag feature提供了类似的功能)</li></ol><h3 id="监控系统"><a href="#监控系统" class="headerlink" title="监控系统"></a>监控系统</h3><p>系统架构服务化后，服务间调用关系错综复杂，出现问题很难定位。所以监控系统极为重要；而且我们一致认为投入生产前必须有全链路监控。为了快速上线，我们在Dubbo官方提供的监控系统<a href="https://dubbo.incubator.apache.org/">dubbo-monitor</a>和<a href="https://zipkin.io/">zipkin</a>基础上做了部分改造：</p><ol><li>dubbo-monitor官方的方案是将监控数据持久化到磁盘。我们考虑到数据保存到磁盘不方便查询，而且多个节点之间共享磁盘不是一个好的方案。所以，我们将数据持久化到MySql。</li><li>配置信息接入<a href="https://github.com/ctripcorp/apollo">Apollo</a> 。</li><li>接入我们现有的报警系统；将监控数据实时上报到我们的报警系统 。</li><li><a href="https://zipkin.io/">zipkin</a>的链路跟踪信息接入我们的日志系统 </li><li>打通全链路监控,从web系统到最后端基础服务调用关系 </li></ol><h3 id="限流降级"><a href="#限流降级" class="headerlink" title="限流降级"></a>限流降级</h3><p>我们采用阿里开源的<a href="https://github.com/alibaba/Sentinel">Sentinel</a>作为限流降级组件。<a href="https://github.com/alibaba/Sentinel">Sentinel</a>官方提供了与<a href="http://dubbo.apache.org/">Dubbo</a>集成的适配器，可以最快的速度投入生产使用。但是，由于<a href="https://github.com/alibaba/Sentinel">Sentinel</a>官方默认的限流降级规则是存储在节点内存中的，节点重启后规则会丢失。所以，团队根据官方提供<a href="https://github.com/ctripcorp/apollo">Apollo</a>的Datasource做了少许改造，使得规则可以持久化到<a href="https://github.com/ctripcorp/apollo">Apollo</a> 。</p><h3 id="服务拆分"><a href="#服务拆分" class="headerlink" title="服务拆分"></a>服务拆分</h3><p>服务拆分是服务化改造的重点。 通常大家都会根据领域来拆分，而领域的划分可大可小没有一个绝对的标准。拆分太细，服务众多，运维成本较高；拆分太粗发挥不了服务化的优势，因此在服务拆分的时候要根据团队当前的实际情况而定。我们团队在具体执行的过程中采用了领域+上/下层服务的方式拆分。我们各个开发团队的划分就是按照领域划分的，而每个团队都会有底层服务(会提供接口给其它团队)和上层服务(为个自业务服务)；所以，原则上每个团队两个服务，一个底层服务，另一个是上层服务；针对部分关键服务(比如，详情页，订单，支付等..) 有独立的服务 。真正做到独立开发，独立测试，独立发布，独立运维。</p><h3 id="流程规范"><a href="#流程规范" class="headerlink" title="流程规范"></a>流程规范</h3><h4 id="DevOps"><a href="#DevOps" class="headerlink" title="DevOps"></a>DevOps</h4><p>服务化后开发人员的智能发生了变化 :</p><ol><li>开发团队的处警方式有被动告知转变为主动处理，系统的运行状态不仅仅依赖SA，更多依靠开发团队主动关注，SA更多关注系统级别的指标，开发人员必须关注系统，业务等各种指标；以最快的速度对系统的异常作出响应。</li><li>测试任务更多依赖开发人员完成，专职测试人员更多关注自动化和质量。</li><li>上线发布不再由SA直接参与，SA负责提供发布/回滚工具，在质量达标后由开发团队独立完成 。</li></ol><h4 id="上线流程"><a href="#上线流程" class="headerlink" title="上线流程"></a>上线流程</h4><ol><li>效率；之前上线涉及多个团队，依赖Jar包过多，上线过程经常遇到代码冲突，包依赖冲突等问题；排查问题涉及多个团队，耗时长，效率低。服务化后每次上线只涉及自己团队成员，不再有代码冲突和包冲突的问题，效率得到了很大的提升 。</li><li>CI/CD流程的建立；我们在服务化推进的过程中同时建立了初步的CI/CD流程 。</li><li>自动化测试；服务化后上线频率更高，为了保证质量我们开始建立了自动化测试系统。</li></ol><h4 id="编码规范增加如下内容"><a href="#编码规范增加如下内容" class="headerlink" title="编码规范增加如下内容:"></a>编码规范增加如下内容:</h4><ul><li>事务处理</li><li>SQL中多表join</li><li>引用的传递 </li><li>限流异常的处理</li><li>超时异常处理</li><li>单元测试规范 </li></ul><h5 id><a href="#" class="headerlink" title></a></h5><h4 id="发布系统"><a href="#发布系统" class="headerlink" title="发布系统"></a>发布系统</h4><ol><li>流量切换方式；服务化之前我们大部分应用都是web应用；系统发布过程中流量的切换是通过Nginx完成的。服务化后流量切换需要依赖服务发现(我们用<a href="http://zookeeper.apache.org/">Zookeeper</a>作为<a href="http://dubbo.apache.org/">Dubbo</a>服务发现的组件)组件完成；因此我们增加了流量切换组件 。</li><li>回滚方式：服务化后系统上线频率变高；同时意味着回滚的频率也会变高。考虑到回滚应用的时候如果需要回滚配置必须手工完成，这样效率并不高。因此，我们根据利用<a href="https://github.com/ctripcorp/apollo">Apollo</a>的OpenAPI真正做到了一键回滚。</li></ol><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>接下来，我们会聚焦如下方面：</p><ul><li>分库分表</li><li>前/中台战略</li><li>容器化</li><li>自动化测试的加强</li><li>监控系统的加强，缩短问题排查的时间</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>给出一个「可落地的方案」能真正体现一个架构师实力；架构师遇到的很多问题都不是技术问题，只是用技术手段解决业务问题，流程问题，质量问题。架构师给出的每一个方案，必须立足于团队的实际情况，实际情况包括但不限于：成本，时间，团队能力，等。而且时刻关注执行结果；如果没有到达预期效果或者在执行过程中偏离方向，尽快根据团队实际情况调整方案。总之，所有的方案都是冲着三个目标 <strong>效率</strong>，<strong>质量</strong>，<strong>成本</strong> 。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;目前我们大部分业务都接入了服务化，在过去将近一年的时间里，我们踩了很多坑，也出现了几次生产事故，同时，从某种意义上讲，我们也做了某些「微创新」，使得架构更适合我们团队的实际情况。从目前的结果来看，基本达成了预定目标；&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>如何提一个问题</title>
    <link href="https://www.yzhu.name/2018/12/23/how-to-question/"/>
    <id>https://www.yzhu.name/2018/12/23/how-to-question/</id>
    <published>2018-12-23T13:01:41.000Z</published>
    <updated>2022-01-19T07:06:58.613Z</updated>
    
    <content type="html"><![CDATA[<p>在我们日常生活以及工作中经常会遇到提问题, 我们有没有想过如何提一个好问题？也许有人会说，「提问谁不回？会讲话的小孩子就会问为什么，这有什么值得说的？」没错，从这个角度上说机会每个人都会提问。但是，我们有没有想过，我们问问题的目的是什么？是得到对问题的解答。大家有没有遇到过这样的情况，自己问题发给对方后始终得不到回复，或者对方敷衍了事，更有甚者被对方怼回去。如果遇到这些情况，我们应该好好反思一下，我们是不是提了一个好问题；在说好问题之前结合个人的经历谈谈什么样的问题不是一个好问题。<span id="more"></span></p><ol><li>提问者在im上发一个截图，截图里面是一个异常栈，没有任何上下文，然后就没有然后了；解答者在那里等对方进一步的信息，结果什么也等到；过了很久(估计等很久没回复)，亲自过来说「帮我看看我的问题」；这个时候才开始沟通，问题是怎么出现的，什么环境，上下文，等… </li><li>有些问题在技术文档，产品文档上都已经说的很清楚；但提问者不看文档，直接把问题抛过来；也许是提问者自己没有按照文档要求操作，或者真的是一个问题但文档FAQ部分已经解释的很清楚 ；</li><li>提问者直接将问题抛出来，不做任何思考和尝试，只在那里等答案；不知道提问者有没有想过，解答者也要花费大量的时间去调研；</li><li>提问者提问道时候，发的关键信息是一张截图，截图中可能有id之类的标示，此时解答者可能需要根据id搜索，只能手工输入，这样既浪费时间也容易出错；</li></ol><p>对于以上的案例，个人觉得提问者需要加强独立思考的能力，不然会给对方留下「伸手党」的感觉；个人觉得可以按照如下的方式考虑提问:</p><ol><li>对于技术问题或者产品问题，一般作者会提供相应的文档；首先我们应该阅读相关文档，特别注意FAQ部分；</li><li>善于使用搜索引擎，具备基本的搜商(SQ) ,推荐用google(对技术工作者，这个不需要解释), 百度请忽略所有「广告」字样的文章，微信，知乎等工具;</li><li>在以上两步都没有找到答案情况下，可以参考第二步搜索过程中出现的文章以及官方提供的文档尝试；比如是否可以升级软件版本，是否可以调整某些参数；相信在尝试过程中会加深自己对问题的理解以及提升自己解决问题的能力</li><li>经过以上步骤能解决95%的问题了；如果还没有解决，这时候再提问，相信这个时候提出的问题质量是非常高的；比如: 该问题是如何出现的，什么环境，硬件型号是什么，软件版本是什么，我的操作步骤是什么，我做了哪些调研，尝试了哪些方式，得到结果是什么,等.. </li></ol><p>那么提一个好问题有那么重要吗？我认为提一个好问题比解答一个问题更能体现一个人的真实水平，因为这证明提问者有独立思考的能力；比如在职场中一个常见的场景是这样的，面试最后部分面试官会问面试者有什么问题吗？这个时候如果提一个高质量的问题，相信会加深面试官的印象,在同等条件的候选者中相信胜出的概率会更高;</p><p>最后关于什么是好问题，这里引用某大佬的一句话 「什么是好问题，你对某个领域有一定研究，你已经翻阅和查询了足够的资料，然后你基于你的理解，对这些资料中，某几个无法理解的细节提出来，希望专业人士给与解惑，而这个细节问题确实具有足够代表性，是理解一些关键问题的钥匙，这就是好问题。」</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在我们日常生活以及工作中经常会遇到提问题, 我们有没有想过如何提一个好问题？也许有人会说，「提问谁不回？会讲话的小孩子就会问为什么，这有什么值得说的？」没错，从这个角度上说机会每个人都会提问。但是，我们有没有想过，我们问问题的目的是什么？是得到对问题的解答。大家有没有遇到过这样的情况，自己问题发给对方后始终得不到回复，或者对方敷衍了事，更有甚者被对方怼回去。如果遇到这些情况，我们应该好好反思一下，我们是不是提了一个好问题；在说好问题之前结合个人的经历谈谈什么样的问题不是一个好问题。&lt;/p&gt;</summary>
    
    
    
    
    <category term="职场" scheme="https://www.yzhu.name/tags/%E8%81%8C%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>一次OOM引发服务雪崩的思考</title>
    <link href="https://www.yzhu.name/2018/12/01/OOM-Casue-Snowslide/"/>
    <id>https://www.yzhu.name/2018/12/01/OOM-Casue-Snowslide/</id>
    <published>2018-12-01T08:36:04.000Z</published>
    <updated>2022-01-19T07:06:58.606Z</updated>
    
    <content type="html"><![CDATA[<p>前几天核心基础服务(简称S,两节点)发生了一次线上故障，导致整个基础服务雪崩，最终，该基础服务故障导致整站多项功能失效；</p><span id="more"></span><h3 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h3><ol><li>4:15 开始S的多个上游服务开始出现大量超时,而且出现告警</li><li>4:30 S的节点S1出现OOM停止服务 ,同时告警系统发出了告警信息</li><li>4:50 S的节点S2由于操作系统内存不足被系统kill ,同时告警系统发出了告警信息<br> ….</li><li>8:00 运营同学反馈问题,技术同学开始排查</li><li>8:05 技术同学发现S故障,重启S服务，服务正常 </li></ol><h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><ol><li>S的上游应用某定时任务在凌晨4:10请求了大量(一次超过200w)的数据将S1先拖死，导致流量全部到S2，S2逐渐不堪重负,导致被系统kill </li><li>S对上游数据请求量没有限制，没有启用限流分组功能</li><li>告警信息没有引起足够重视,告警被忽略</li><li>告警信息仅仅报告给monitor，没有到系统的owner</li></ol><h3 id="后续措施"><a href="#后续措施" class="headerlink" title="后续措施"></a>后续措施</h3><ol><li>启用限流，分组功能</li><li>必须严格执行code review</li><li>告警信息必须分级，而且必须同时报告给monitor和系统owner;接警人接到报警后根据不同的级别进行处理</li></ol><h3 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h3><p>在互联网领域我们为了追求n个9，会采取一系列的流程措施来保障系统的可用性;但，真正落实到位的有多少? 其实，线上故障不可怕，可怕的是没有相应的应对措施；更可怕的是有相应的应对措施，但没有落实到位; 比如: 在服务化推进的过程中架构团队已经提供了限流，分组等功能，而且也进行了宣讲；但，最终这些措施还是没有落实到位；也许这就是大厂和创业公司的区别，大厂里面的架构师大部分也是螺丝钉，只要自己有产出符合预期完成KPI就是一个合格的架构师；但是，创业公司的架构师要考虑更多的因素，从最初的技术方案到设计，实现，最终的落地执行都需要全程参与确保每一个功能真正落实到位</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前几天核心基础服务(简称S,两节点)发生了一次线上故障，导致整个基础服务雪崩，最终，该基础服务故障导致整站多项功能失效；&lt;/p&gt;</summary>
    
    
    
    
    <category term="线上问题" scheme="https://www.yzhu.name/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>服务化-服务拆分</title>
    <link href="https://www.yzhu.name/2018/09/24/Service-Split/"/>
    <id>https://www.yzhu.name/2018/09/24/Service-Split/</id>
    <published>2018-09-24T01:04:29.000Z</published>
    <updated>2022-01-19T07:06:58.612Z</updated>
    
    <content type="html"><![CDATA[<p>服务化的过程中必然会面对服务拆分的问题；拆分粒度太粗不能体现服务化的优势，拆分过细会导致各项成本过高；所以架构师在服务拆分时要权衡各方面的利弊根据当前情况做出最优解;以下内容是我们团队在使用dubbo的过程，根据实际情况考虑的服务拆分方式；<span id="more"></span></p><h3 id="拆分考虑的因素"><a href="#拆分考虑的因素" class="headerlink" title="拆分考虑的因素"></a>拆分考虑的因素</h3><ul><li>业务领域</li><li>上下层级调用关系</li><li>技术因素</li><li>成本</li></ul><p>我们在拆分过程中充分考虑以上4个因素；特别是在业务领域方面有较多的争议；业务领域下可以继续分子领域，所以在实际方案设计过程中部分团队服务拆分太细；经过多次沟通后我们将部分模块合并为一个provider，因为拆分过细对我们会有如下问题：</p><ul><li>目前我们数据库没有拆分(下阶段会根据个业务线拆库)，服务过细会增加数据库的连接数量</li><li>增加了运维成本</li><li>增加硬件成本</li></ul><h3 id="拆分原则"><a href="#拆分原则" class="headerlink" title="拆分原则"></a>拆分原则</h3><ul><li>模块按照子业务领域和上下层级拆分</li><li>服务调用只能上层服务调用下层服务</li><li>禁止provider之间循环调用</li><li>相同层级的模块可以作为同一个服务对外发布</li><li>模块可以低成本的拆分为一个独立的provider</li></ul><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">                        ___Api</span><br><span class="line">                       |</span><br><span class="line">          ___Model_A--&gt;|</span><br><span class="line">          |            |___Impl</span><br><span class="line">          |            </span><br><span class="line">          |</span><br><span class="line">          |              ___Api</span><br><span class="line">          |             |</span><br><span class="line">Proider--&gt;|___Model_B--&gt;|</span><br><span class="line">          |             |</span><br><span class="line">          |             |___Impl</span><br><span class="line">          |              </span><br><span class="line">          |             </span><br><span class="line">          |____dubbo_provider.xml   </span><br></pre></td></tr></table></figure><h4 id="模型特点"><a href="#模型特点" class="headerlink" title="模型特点"></a>模型特点</h4><ul><li>Model_A和Model_B必须是同一层级的模块，比如: 同为数据访问层或者同为业务服务层</li><li>对外仅提供各个model的Api</li><li>同一个服务可以发布多个模块,节省成本</li><li>服务调用只能是上层服务调用下层服务，从而避免服务循环调用</li><li>各Model可以低成本的拆分为独立的provider</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;服务化的过程中必然会面对服务拆分的问题；拆分粒度太粗不能体现服务化的优势，拆分过细会导致各项成本过高；所以架构师在服务拆分时要权衡各方面的利弊根据当前情况做出最优解;以下内容是我们团队在使用dubbo的过程，根据实际情况考虑的服务拆分方式；&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>基于配置中心属性,Dubbo Consumer动态切换调用RPC/本地方法</title>
    <link href="https://www.yzhu.name/2018/07/06/Dubbo-Local-RPC-Switch/"/>
    <id>https://www.yzhu.name/2018/07/06/Dubbo-Local-RPC-Switch/</id>
    <published>2018-07-06T06:11:48.000Z</published>
    <updated>2022-01-21T07:37:39.119Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求缘由"><a href="#需求缘由" class="headerlink" title="需求缘由"></a>需求缘由</h2><p>最近部分只读服务已经切换到了Dubbo提供的服务化接口；经过一段时间的生产验证, 各种监控指标显示非常稳定，所以打算开始切换写服务接口; 但大家对写操作跑服务化没有信心，于是提了一个需求能随时将远程rpc调用切换打本地jar方法调用；<span id="more"></span></p><h2 id="dubbo的stub"><a href="#dubbo的stub" class="headerlink" title="dubbo的stub"></a>dubbo的stub</h2><p>听到这个需求，马上想到dubbo的stub属性；但需要在原来代码中做部分改造,具体步骤是：</p><ol><li>原来的jar包实现方法中增加带自身接口类型参数的构造函数;</li><li>在所有的接口实现方法中判断开关状态决定调用RPC/本地方法;<br>但是，这样对原代码有极强入侵性；我们认为这不是最好的方案，于是有了下面的方案；</li></ol><h2 id="扩展dubbo-reference标签属性"><a href="#扩展dubbo-reference标签属性" class="headerlink" title="扩展dubbo:reference标签属性"></a>扩展dubbo:reference标签属性</h2><p>首先需要说明，我们在实际使用dubbo的过程中并没有直接使用dubbo这个命名空间，而是自定义了自己的命名空间，自定义命名空间兼容了dubbo命名空间的所有属性，而且扩展了自己的属性；于是针对该需求我们有了新的方案，具体流程如下 ：</p><ol><li>增加<strong>reference</strong>的属性<strong>localbean</strong>，这个属性值为一个本地bean的id；类似的xml配置如下：<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;localDemoService&quot;</span> <span class="attr">class</span>=<span class="string">&quot;com.example.Service.impl.DemoServiceImpl&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mydubbo:reference</span> <span class="attr">interface</span>=<span class="string">&quot;com.example.Service.DemoService&quot;</span> <span class="attr">id</span>=<span class="string">&quot;remoteDemoService&quot;</span></span></span><br><span class="line"><span class="tag">                 <span class="attr">localbean</span> =<span class="string">&quot;localDemoService&quot;</span> <span class="attr">version</span>=<span class="string">&quot;1.0&quot;</span>/&gt;</span></span><br></pre></td></tr></table></figure></li><li>继承ReferenceBean实现自定义属性locbean；</li><li>继承DubboNamespaceHandler重写init方法,如下:<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.init();</span><br><span class="line">        <span class="keyword">this</span>.registerBeanDefinitionParser(<span class="string">&quot;reference&quot;</span>, <span class="keyword">new</span> MyBeanDefinitionParser(LocalReferenceBean.class, <span class="keyword">false</span>));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li>自定义filter<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RPCLocalSwitchFilter</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">Filter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(RPCLocalSwitchFilter.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> ProxyFactory PROXY_FACTORY = (ProxyFactory)ExtensionLoader.getExtensionLoader(ProxyFactory.class).getAdaptiveExtension();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">RPCLocalSwitchFilter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Result <span class="title">invoke</span><span class="params">(Invoker&lt;?&gt; invoker, Invocation invocation)</span> <span class="keyword">throws</span> RpcException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(cfg.getBooleanProperty(<span class="string">&quot;&quot;</span>)) &#123;  <span class="comment">//判断配置中心开工状态</span></span><br><span class="line">            Object bean = <span class="keyword">this</span>.getLocalBean(invoker);</span><br><span class="line">            <span class="keyword">return</span> bean != <span class="keyword">null</span>?<span class="keyword">this</span>.executeLocalMethod(bean, invoker, invocation):invoker.invoke(invocation);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> invoker.invoke(invocation);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Result <span class="title">executeLocalMethod</span><span class="params">(T bean, Invoker&lt;?&gt; invoker, Invocation invocation)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(logger.isDebugEnabled()) &#123;</span><br><span class="line">            logger.debug(<span class="string">&quot;execute local method instead of rpc &quot;</span> + bean.getClass().getCanonicalName() + <span class="string">&quot;.&quot;</span> + invocation.getMethodName());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Invoker localInvoker = PROXY_FACTORY.getInvoker(bean, invoker.getInterface(), invoker.getUrl());</span><br><span class="line">        <span class="keyword">return</span> localInvoker.invoke(invocation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> &lt;T&gt; <span class="function">T <span class="title">getLocalBean</span><span class="params">(Invoker&lt;?&gt; invoker)</span> </span>&#123;</span><br><span class="line">        String refLocal = invoker.getUrl().getParameter(<span class="string">&quot;localBean&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span>(refLocal != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                ApplicationContext e = LocalReferenceBean.getSpringContext();</span><br><span class="line">                <span class="keyword">return</span> e.getBean(refLocal, invoker.getInterface());</span><br><span class="line">            &#125; <span class="keyword">catch</span> (BeansException e) &#123;</span><br><span class="line">                logger.error(var4.getMessage() + <span class="string">&quot; couldn\&#x27;t find the bean[&quot;</span> + localBean + <span class="string">&quot;], please check&quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>配置filter<br>在resources/META-INF/dubbo/com.alibaba.dubbo.rpc.Filter文件中增加如下内容:<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reflocal=com.example.filter. RPCLocalSwitchFilter</span><br></pre></td></tr></table></figure>配置consumer的filter属性<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mydubbo:consumer</span> <span class="attr">filter</span>=<span class="string">&quot;reflocal&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure>至此可以通过配置中的开关状态来控制远程RPC/本地jar方法调用 ;</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;需求缘由&quot;&gt;&lt;a href=&quot;#需求缘由&quot; class=&quot;headerlink&quot; title=&quot;需求缘由&quot;&gt;&lt;/a&gt;需求缘由&lt;/h2&gt;&lt;p&gt;最近部分只读服务已经切换到了Dubbo提供的服务化接口；经过一段时间的生产验证, 各种监控指标显示非常稳定，所以打算开始切换写服务接口; 但大家对写操作跑服务化没有信心，于是提了一个需求能随时将远程rpc调用切换打本地jar方法调用；&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    <category term="Dubbo" scheme="https://www.yzhu.name/tags/Dubbo/"/>
    
  </entry>
  
  <entry>
    <title>服务化准备工作</title>
    <link href="https://www.yzhu.name/2018/06/18/Parepare-Service/"/>
    <id>https://www.yzhu.name/2018/06/18/Parepare-Service/</id>
    <published>2018-06-18T03:56:19.000Z</published>
    <updated>2022-01-19T07:06:58.606Z</updated>
    
    <content type="html"><![CDATA[<p>在决定进行服务化时，我们做了一些必要的准备工作；从服务治理，链路跟踪，监控以及报警方面到代码层面，运维，流程等多个方面做了相应的调整</p><span id="more"></span><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>我们的服务化是在dubbo的基础上进行的，首先适配了服务化需要的相关组件，包括配置中心，链路跟踪，监控，服务治理等，整体架构如图所示 </p><p><img src="/2018/06/18/Parepare-Service/arch.jpg" alt="架构图"></p><ul><li>配置中心: Apollo </li><li>链路跟踪: Zipkin, Kafka, ES</li><li>服务治理: dubbo官方的dubbo-admin</li><li>服务监控: dubbo官方的dubbo-monitor</li></ul><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="服务化SDK"><a href="#服务化SDK" class="headerlink" title="服务化SDK"></a>服务化SDK</h3><p>我们提供了服务化SDK给业务团队使用；SDK自定义了xsd文件继承了dubbo的xml标签，在此基础上集成了apollo配置；这样业务团队在使用的过程中对一些系统配置不需要关注，比如注册中心，启用监控，启用链路跟踪等 .</p><h3 id="代码改造"><a href="#代码改造" class="headerlink" title="代码改造"></a>代码改造</h3><ol><li>分离Service的API接口到独立的Jar包</li><li>检查接口的输入输出参数是否实现了Serializable接口</li><li>是否使用了接口参数在方法内部修改后的值，代码如下:  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*服务端*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">call</span><span class="params">(List&lt;String&gt; list)</span></span>&#123;</span><br><span class="line">        list.add(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*客户端*/</span> </span><br><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> ArrayList();</span><br><span class="line">service.call(list);</span><br><span class="line">list.get(<span class="number">0</span>)</span><br></pre></td></tr></table></figure> 这个代码在同一个JVM是没有问题，但RPC调用并不能返还预期的结果；</li><li>本地事务改造为分布式事务</li><li>确保所有的单元测试通过</li></ol><h3 id="日志输出增加链路跟踪信息"><a href="#日志输出增加链路跟踪信息" class="headerlink" title="日志输出增加链路跟踪信息"></a>日志输出增加链路跟踪信息</h3><p>每条输出日志包括了zipkin的span id,这样每条日志都能清楚的跟踪到从整个调用链；</p><h3 id="监控系统"><a href="#监控系统" class="headerlink" title="监控系统"></a>监控系统</h3><p>dubbo-monitor对接现有的报警系统，将采集到监控数据上报到现有报警系统,做到实时报警 </p><h3 id="发布系统"><a href="#发布系统" class="headerlink" title="发布系统"></a>发布系统</h3><p>dubbo的服务在正常停止的过程中不再接受新的请求,但是启动后在注册中心完成注册马上开始接受新的请求；我们希望启动后由发布系统控制什么时候开始接受新请求；</p><h3 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h3><p>服务化后对所有的依赖必须基于强版本，所以对每一个发布的服务都提供了强版本策略，而且正常情况下保证每个新版本必须兼容旧版本；版本路线如图所示:<br><img src="/2018/06/18/Parepare-Service/version.jpg" alt="版本"></p><h3 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h3><p>  改造现有的流程控制系统，做到每个团队可以独立开发，独立测试，独立部署，独立上线以及独立回滚; 而且保证CI, CD能正常工作,所有流程能自动完成; </p><h3 id="服务拆分"><a href="#服务拆分" class="headerlink" title="服务拆分"></a>服务拆分</h3><p>首先拆分出基础服务,包括地址服务，推送服务，图片服务，等与业务无关的服务；其次按照业务领域进行拆分，包括订单，用户中心，支付等</p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><ul><li>服务安全</li><li>物理分组 </li><li>多维度监控</li><li>等…</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在决定进行服务化时，我们做了一些必要的准备工作；从服务治理，链路跟踪，监控以及报警方面到代码层面，运维，流程等多个方面做了相应的调整&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>分布式事务实现</title>
    <link href="https://www.yzhu.name/2018/05/22/distributed-transaction/"/>
    <id>https://www.yzhu.name/2018/05/22/distributed-transaction/</id>
    <published>2018-05-22T07:17:36.000Z</published>
    <updated>2022-01-21T07:37:39.120Z</updated>
    
    <content type="html"><![CDATA[<p>在分布式系统中，同时满足“CAP定律”中的“一致性”、“可用性”和“分区容错性”三者是不可能的。在互联网领域的绝大多数的场景，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。<span id="more"></span></p><h3 id="XA"><a href="#XA" class="headerlink" title="XA"></a>XA</h3><p>XA规范主要定义了事务管理器(Transaction Manager)和资源管理器(Resource Manager)之间的接口. XA引入事务管理器是因为在分布式系统中，从理论上讲两台机器上无法达到一致的状态，需要引入一个外部点进行协调。事务管理器控制着全局事务，管理事务生命周期，并协调资源, 资源管理器负责控制和管理实际资源 。XA是一个两阶段提交协议，该协议分为以下两个阶段：</p><ol><li> 事务协调器要求每个涉及到事务的数据库预提交，并反映是否可以提交 。</li><li>事务协调器要求每个数据库提交数据 。</li></ol><p>如果有任何一个数据库否决此次提交，那么所有数据库都会被要求回滚它们在此事务中的那部分信息 。</p><h3 id="JTA"><a href="#JTA" class="headerlink" title="JTA"></a>JTA</h3><p>JTA作为JAVA平台上的事务规范，同时定义了对XA事务的支持；在JTA中，事务管理器抽象为javax.transaction.TransactionManager接口，通过底层事务服务（即JTS）实现,JTA仅仅定义了接口，具体的实现则是由供应商(如J2EE厂商)负责提供，目前JTA的实现主要由J2EE容器所提供的JTA实现(如JBOSS)和独立的JTA实现(如JOTM，Atomikos)。</p><blockquote><p>JTA本质上是两阶段提交，实现复杂，牺牲了可用性，对性能影响较大, 适合对数据强一致(<strong>其实也不能100%保证强一致</strong>)要求很高的关键领域; 大部分互联网业务都不会采用两阶段提交的方式 。</p></blockquote><h3 id="链式事务管理"><a href="#链式事务管理" class="headerlink" title="链式事务管理"></a>链式事务管理</h3><p>这种方式也Spring提供的，可以将两个或多个数据库资源的事务串联到一起，公用一个TransactionManager来实现对多个资源的事务。配置方式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.data.transaction.ChainedTransactionManager&quot;&gt;</span><br><span class="line">  &lt;property name=&quot;transactionManagers&quot;&gt;</span><br><span class="line">    &lt;list&gt;</span><br><span class="line">      &lt;bean</span><br><span class="line">        class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;</span><br><span class="line">        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource1&quot; /&gt;</span><br><span class="line">      &lt;/bean&gt;</span><br><span class="line">      &lt;bean</span><br><span class="line">        class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;</span><br><span class="line">        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource2&quot; /&gt;</span><br><span class="line">      &lt;/bean&gt;</span><br><span class="line">    &lt;/list&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line"> &lt;/code&gt;</span><br></pre></td></tr></table></figure><p>针对多个数据库实现事务。使用这种方式时，在Spring事务提交的时候，它会依次调用里面的多个dataSource的commit()方法，如果业务方法出错，就会按照相反的顺序调用rollback()方法。这种方法可能会出现先前的提交成功，之后的提交失败，所以还是会有事务失败的可能。</p><blockquote><p>实现简单，但可能会出现先前的提交成功，之后的提交失败，所以还是会有事务失败的可能</p></blockquote><h3 id="最大努力一次提交-Best-Efforts-1PC"><a href="#最大努力一次提交-Best-Efforts-1PC" class="headerlink" title="最大努力一次提交(Best Efforts 1PC)"></a>最大努力一次提交(Best Efforts 1PC)</h3><p>在一个系统中使用数据库和带事务功能的消息中间件，业务流程如下</p><ol><li>开始消息事务</li><li>发送消息</li><li>开始数据库事务</li><li>更新数据库</li><li>提交数据库事务</li><li>提交消息事务</li></ol><p>有两个事务，分别是DB的和JMS的事务，事务的开启和提交都是相互独立的。依次提交这两个事务，只要第二个事务顺利提交，整个方法就能够保证数据的一致性。实际上，在绝大多数情况下，只要数据库和MQ能够正常访问，这也确实能够保证。所以，这种方式就叫’最大努力’一次提交。</p><p>使用这种方式，事物提交的顺序是非常重要的。假设在提交messaging transaction的时候发生错误，这时数据库的事务已经提交，无法回滚，但是消息的事务被回滚，那么这一条消息会被重新放回队列中，该业务方法会被再次触发，再次在一个新的事务中处理。但是，这时数据的处理已经完成，只是最后JMS的事物提交出错，那么就需要通过防止重复提交的方式，来避免数据库的再次处理。</p><h3 id="事务补偿型-TCC型事务–Try-Confirm-Cancel"><a href="#事务补偿型-TCC型事务–Try-Confirm-Cancel" class="headerlink" title="事务补偿型(TCC型事务–Try/Confirm/Cancel)"></a>事务补偿型(TCC型事务–Try/Confirm/Cancel)</h3><p>其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段：</p><ul><li>Try 阶段主要是对业务系统做检测及资源预留</li><li>Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的, 即：只要Try成功，Confirm一定成功</li><li>Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放</li></ul><p>在一个长事务，一个由两台服务器一起参与的事务，服务器A发起事务，服务器B参与事务，B所处理时间可能比较长。如果按照ACID的原则，要保持事务的隔离性、一致性，服务器A中发起的事务中使用到的事务资源将会被锁定，不允许其他应用访问到事务过程中的中间结果，直到整个事务被提交或者回滚。这就造成事务A中的资源被长时间锁定，系统的可用性将不可接受。服务器A的事务如果执行顺利，那么事务A就先行提交，如果事务B也执行顺利，则事务B也提交，整个事务就算完成。但是如果事务B执行失败，事务B本身回滚，这时事务A已经被提交，所以需要执行一个<strong>补偿</strong>操作，将已经提交的事务A执行的操作作反操作，恢复到未执行前事务A的状态。这样的事务模型，是牺牲了一定的隔离性和一致性的，但是提高了事务的可用性。</p><blockquote><p>与两阶段提交相比实现及流程相对简单，但应用层要写很多补偿代码(而且补偿也不能保证一定成功)</p></blockquote><h3 id="本地流水表实现最终一致性"><a href="#本地流水表实现最终一致性" class="headerlink" title="本地流水表实现最终一致性"></a>本地流水表实现最终一致性</h3><p>以电商下单场景为例，主要涉及到两个操作，<strong>扣减库存</strong>和<strong>生成订单</strong>,因为两个操作在不同的数据库，无法保证强一致性,可以通过本地流水表来实现最终一致性 ， 具体流程如下:</p><ul><li>生成交易操作唯一标示token</li><li>事务一(库存系统): <ul><li>冻结库存</li><li>根据下单流水号生成商品的库存冻结记录,冻结记录主要包括<strong>skuId</strong>,<strong>token</strong>,<strong>冻结数量</strong>,<strong>状态</strong> .状态有3种状态: <strong>已冻结</strong>，<strong>下单成功扣减</strong>，<strong>下单失败释放</strong>,初始状态为已冻结</li></ul></li><li>如果事务一失败，直接返回；如果成功进入事务二 </li><li>事务二(订单系统, 本地事务)：根据token生成订单,订单的状态主要包括：<strong>未支付</strong>,<strong>已支付</strong>,<strong>超时未支付</strong>,订单的初始状态为<strong>未支付</strong></li><li>事务二如果成功，则进行后续的流程,</li><li>事务二如果失败，调用库存系统的回滚接口，返回下单失败;</li><li>定时任务: 因为存在<strong>事务一成功</strong>而<strong>事务二</strong>失败的情况，这样会冻结商品的部分库存，所以可以捞取出创建超过一定时间状态为<strong>已冻结</strong>的所有冻结记录，根据每个冻结记录的token去订单表查询，若不存在对应的订单，则将冻结记录的状态更新为<strong>下单失败释放</strong>，并回滚商品库存数量</li></ul><h3 id="异步确保型"><a href="#异步确保型" class="headerlink" title="异步确保型"></a>异步确保型</h3><p>将一些有同步的事务操作变为异步操作，避免对数据库事务的争用；继续以以电商下单场景为例，支付成功后增加用户积分；</p><ul><li>事务一(订单系统)，订单状态修改为支付成功，发送支付成功消息</li><li>事务二(用户系统)，用户系统接到支付成功消息后，增加用户积分</li></ul><h3 id="MQ事务消息"><a href="#MQ事务消息" class="headerlink" title="MQ事务消息"></a>MQ事务消息</h3><p>一些第支持事务消息MQ，比如RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，其思路大致为：</p><ul><li>第一阶段Prepared消息，会拿到消息的地址。</li><li>第二阶段执行本地事务，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。</li></ul><p>在业务方法内要想消息队列提交两次请求，一次发送消息和一次确认消息。如果确认消息发送失败了RocketMQ会定期扫描消息集群中的事务消息，这时候发现了Prepared消息，它会向消息发送者确认，是回滚还是继续发送确认消息。这样就保证了消息发送保证与本地事务同时成功或同时失败 </p><h3 id="分布式事务实现的原则"><a href="#分布式事务实现的原则" class="headerlink" title="分布式事务实现的原则"></a>分布式事务实现的原则</h3><ul><li>大事务拆成小事务，每个小事务都是单机上的事务</li><li>补偿 + 重试， 业务上设计补偿机制，而且保证补偿失败后有重试机制</li><li>幂等, 保证每次事务操作是幂等的，保证幂等的方式可以采用：<ul><li>状态值，每次写操作的时候检查状态值</li><li>唯一标示，每次写操作都带入业务唯一标示</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在分布式系统中，同时满足“CAP定律”中的“一致性”、“可用性”和“分区容错性”三者是不可能的。在互联网领域的绝大多数的场景，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    <category term="分布式" scheme="https://www.yzhu.name/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>微服务框架选型之 Dubbo VS. Spring Cloud</title>
    <link href="https://www.yzhu.name/2018/04/30/Microservice-Dubbo-VS-Spring-Cloud/"/>
    <id>https://www.yzhu.name/2018/04/30/Microservice-Dubbo-VS-Spring-Cloud/</id>
    <published>2018-04-30T01:26:01.000Z</published>
    <updated>2022-01-19T07:06:58.601Z</updated>
    
    <content type="html"><![CDATA[<p>当前开源的微服务框架有Dubbo, Spring Cloud, Dubbox, Motan, Thrift, GRPC 等；以Dubbo和Spring Cloud使用最广，本文仅对这两个框架结合自己项目的情况进行比较。<span id="more"></span></p><h2 id="Dubbo-架构"><a href="#Dubbo-架构" class="headerlink" title="Dubbo 架构"></a>Dubbo 架构</h2><p><img src="/2018/04/30/Microservice-Dubbo-VS-Spring-Cloud/dubbo-architecture.png" alt="Dubbo 架构图"></p><p>从Dubbo的架构图中我们可以看出Dubbo重点是RPC框架,对应微服务相关的其它组件,如服务发现,配置管理等需要用户自己适配;</p><h2 id="Spring-Cloud架构"><a href="#Spring-Cloud架构" class="headerlink" title="Spring Cloud架构"></a>Spring Cloud架构</h2><p><img src="/2018/04/30/Microservice-Dubbo-VS-Spring-Cloud/diagram-distributed-systems.svg" alt="Spring Cloud架构图"></p><p>Spring Cloud架构中包括了微服务生态中的大部分组件，包括但不限于服务发现，配置管理，路由网关,限流熔断等</p><h2 id="详细比较"><a href="#详细比较" class="headerlink" title="详细比较"></a>详细比较</h2><p>我们结合自己项目的情况从10个方面对两个框架比较</p><h3 id="迁移成本"><a href="#迁移成本" class="headerlink" title="迁移成本"></a>迁移成本</h3><p>  现有的系统正在线上运行，所以迁移过程中不能影响现有的系统而且尽量不耽误新功能的开发；</p><ol><li>Dubbo迁移过程</li></ol><ul><li>分离服务接口到独立的jar包</li><li>在服务端xml文件中配置服务接口将服务暴露出去</li><li>在客户端引入接口jar包，在xml文件中引入服务端暴露的接口</li></ul><ol start="2"><li>Spring Cloud迁移过程</li></ol><ul><li>为接口定义HTTP协议 </li><li>服务端在controller里面以rest API的方式暴露接口</li><li>客户端根据定义的http协议用FeignClient注解的形式定义访问接口</li><li>客户端代码引入FeignClient定义的协议接口</li></ul><p>由此可见，dubbo迁移不需要修改java 代码，spring cloud 需要为每个接口定义一套http协议，而且客户端需要定义访问接口</p><h3 id="社区支持"><a href="#社区支持" class="headerlink" title="社区支持"></a>社区支持</h3><p>社区支持我们参考github的数据</p><table><thead><tr><th></th><th>Dubbo</th><th>Spring Cloud</th></tr></thead><tbody><tr><td>Star</td><td>18k+</td><td>2.1k+</td></tr><tr><td>Fork</td><td>12k+</td><td>1.1k+</td></tr><tr><td>Contribute</td><td>70+</td><td>130+</td></tr><tr><td>Open Issues</td><td>430+</td><td>380+</td></tr><tr><td>Close Issues</td><td>740+</td><td>1.9k+</td></tr><tr><td>Latest Updated</td><td>几个小时前，但是2014-10-30到2017-09-07断更</td><td>几个小时前</td></tr></tbody></table><h3 id="服务治理"><a href="#服务治理" class="headerlink" title="服务治理"></a>服务治理</h3><ul><li>Dubbo服务治理不太完善</li><li>Spring Cloud有比较完善的服务治理组件,Zuul, Ribbon, Hystrix等</li></ul><h3 id="生态建设"><a href="#生态建设" class="headerlink" title="生态建设"></a>生态建设</h3><ul><li>Dubbo生态包括:   RPC框架，服务治理, 服务降级，其它相关组件可以集成第三方成熟的开源组件；</li><li>Spring Cloud生态包括: RPC框架，服务发现，配置服务，服务治理，服务降级，日志收集，任务管理，几乎包括了服务化相关的所有组件；</li></ul><h3 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h3><p>  Dubbo和Spring Cloud 都没有实现分布式事务 </p><h3 id="运维成本"><a href="#运维成本" class="headerlink" title="运维成本"></a>运维成本</h3><p>  运维需要关注以下组件</p><ol><li>服务发现</li></ol><ul><li>Dubbo采用Zookeeper公司有Zookeeper的使用经验</li><li>Spring Cloud采用Eureka,需要搭建新的eureka服务</li></ul><ol start="2"><li>配置管理</li></ol><ul><li>Dubbo采用Apollo，公司已经投入生产环境</li><li>Spring Cloud 需要重新部署spring cloud config </li></ul><ol start="3"><li>服务治理</li></ol><ul><li>Dubbo 需要独立部署Dubbo-OPS, Zipkin/hydp/cat</li><li>Spring Cloud 需要独立部署 Zuul, Zipkin<br>综上，Dubbo新部署2个组件，Sping Cloud新部署4个组件</li></ul><h3 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h3><ul><li>Dubbo开发流程<ol><li>确保服务发现组件已启动</li><li>定义接口，生成接口jar包发布到maven仓库</li><li>服务端实现接口逻辑；同时客户端引入接口jar包,实现客户端逻辑, 完成自测</li><li>客户端服务端联调</li></ol></li><li>Spring Cloud开发流程<ol><li>确保服务发现组件已启动</li><li>定义http接口，形成文档</li><li>服务端实现接口逻辑而且以rest API的形式发布出去；同时客户端根据http接口约定实现客户端逻辑，完成自测</li><li>客户端服务端联调<br>所以，Dubbo的接口具有强依赖性，Spring Cloud主要依赖约定</li></ol></li></ul><h3 id="集成测试"><a href="#集成测试" class="headerlink" title="集成测试"></a>集成测试</h3><p>Dubbo和Spring Cloud集成测试的时候都需要服务的和客户端配合完成</p><h3 id="学习成本"><a href="#学习成本" class="headerlink" title="学习成本"></a>学习成本</h3><ul><li>Dubbo主要提供Spring Xml配置的方式，同时社区也提供SpringBoot的注解方式Dubbo Spring Boot Project ，而且Dubbo官方提供了最佳实践；目前我们的项目中两种方式都存在，所以学习成本基本可以忽略；</li><li>Spring Clould是建立在SpringBoot基础上的，采用大量的注解方式，目前我们也大量使用SpringBoot, 所以学习成本基本可以忽略；</li></ul><h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><ul><li>Dubbo默认采用Dubbo协议，Dubbo协议工作在TCP层,同等条件下性能优于HTTP协议</li><li>Spring Clould采用HTTP协议, 性能略逊与Dubbo</li></ul><p>最后将以上比较项目给出数值的形式 </p><table><thead><tr><th>对比项目</th><th>权重</th><th>Dubbo</th><th>Spring Cloud</th></tr></thead><tbody><tr><td>迁移成本</td><td>10</td><td>8</td><td>4</td></tr><tr><td>社区支持</td><td>9</td><td>7</td><td>8</td></tr><tr><td>服务治理</td><td>8</td><td>5</td><td>7</td></tr><tr><td>生态建设</td><td>7</td><td>7</td><td>8</td></tr><tr><td>分布式事务</td><td>6</td><td>0</td><td>0</td></tr><tr><td>运维成本</td><td>5</td><td>4</td><td>3</td></tr><tr><td>开发流程</td><td>4</td><td>3</td><td>2</td></tr><tr><td>集成测试</td><td>3</td><td>2</td><td>2</td></tr><tr><td>学习成本</td><td>2</td><td>2</td><td>1</td></tr><tr><td>性能</td><td>1</td><td>1</td><td>0</td></tr><tr><td>总计</td><td></td><td>37</td><td>33</td></tr></tbody></table><h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h2><p>  Dubbo优势在于从迁移成本低，能在尽可能不改动现有代码的基础上完成服务化的迁移，而且我们团队成员使用Dubbo的经验比使用Spring Cloud的经验丰富；Spring Cloud的优势在于生态比较完善，社区支持良好；最终根据我们项目当前实际情况,我们认为Dubbo是最好的选择，可以在业务开发的同时平滑地完成服务化;</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;当前开源的微服务框架有Dubbo, Spring Cloud, Dubbox, Motan, Thrift, GRPC 等；以Dubbo和Spring Cloud使用最广，本文仅对这两个框架结合自己项目的情况进行比较。&lt;/p&gt;</summary>
    
    
    
    
    <category term="微服务" scheme="https://www.yzhu.name/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    <category term="Dubbo" scheme="https://www.yzhu.name/tags/Dubbo/"/>
    
  </entry>
  
</feed>
